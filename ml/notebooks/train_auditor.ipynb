{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d01be08c",
      "metadata": {
        "id": "d01be08c"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b396df7f",
      "metadata": {
        "id": "b396df7f"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from dataclasses import dataclass\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70c8f5b4",
      "metadata": {
        "id": "70c8f5b4"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeeb49de",
      "metadata": {
        "id": "aeeb49de"
      },
      "outputs": [],
      "source": [
        "RISK_LABELS = [\"scam\", \"adult\", \"illegal\", \"spam\", \"low_info\"]\n",
        "\n",
        "\n",
        "class AuditorModel(nn.Module):\n",
        "    def __init__(self, encoder_name: str, hidden_dim: int = 128):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
        "        emb_dim = self.encoder.config.hidden_size\n",
        "\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "\n",
        "        self.label_head = nn.Linear(hidden_dim, len(RISK_LABELS))\n",
        "        self.score_head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
        "        out = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        pooled = out.last_hidden_state[:, 0]\n",
        "        shared = self.shared(pooled)\n",
        "\n",
        "        label_probs = torch.sigmoid(self.label_head(shared))\n",
        "        risk_score = torch.sigmoid(self.score_head(shared)).squeeze(-1)\n",
        "\n",
        "        return {\n",
        "            \"label_probs\": label_probs,\n",
        "            \"risk_score\": risk_score\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d5528c6",
      "metadata": {
        "id": "3d5528c6"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "febd6dbf",
      "metadata": {
        "id": "febd6dbf"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AuditorSample:\n",
        "    text: str\n",
        "    risk_score: float       # [0, 1] noisy\n",
        "    risk_labels: List[int]  # multi-hot\n",
        "    source: str             # rule / synthetic / self_train\n",
        "\n",
        "\n",
        "class AuditorDataset(Dataset):\n",
        "    def __init__(self, samples: List[AuditorSample], tokenizer, max_len: int = 256):\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        enc = self.tokenizer(\n",
        "            s.text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        risk_labels = torch.tensor(\n",
        "            [s.risk_labels[k] for k in RISK_LABELS],\n",
        "            dtype=torch.float,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"risk_score\": torch.tensor(s.risk_score, dtype=torch.float),\n",
        "            \"risk_labels\": risk_labels,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d951b929",
      "metadata": {
        "id": "d951b929"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5776b1ce",
      "metadata": {
        "id": "5776b1ce"
      },
      "outputs": [],
      "source": [
        "def train_auditor(dataset, encoder_name: str):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = AuditorModel(encoder_name).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "    bce = torch.nn.BCELoss()\n",
        "    mse = torch.nn.MSELoss()\n",
        "    epochs = 10\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        loop = tqdm.tqdm(loader, desc=f\"Epoch {epoch + 1}\")\n",
        "        for batch in loop:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "            )\n",
        "\n",
        "            loss_labels = bce(out[\"label_probs\"], batch[\"risk_labels\"])\n",
        "            loss_score = mse(out[\"risk_score\"], batch[\"risk_score\"])\n",
        "\n",
        "            loss = loss_labels + loss_score\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: loss={epoch_loss / len(loader):.4f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa44e4b",
      "metadata": {
        "id": "caa44e4b"
      },
      "source": [
        "## Generate synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "803f8782",
      "metadata": {
        "id": "803f8782"
      },
      "outputs": [],
      "source": [
        "SCAM_PHRASES = [\n",
        "    \"Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð´Ð¾Ñ…Ð¾Ð´\",\n",
        "    \"Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ð¸Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð¾Ð¿Ð»Ð°Ñ‚Ñƒ\",\n",
        "    \"Ð¸Ð½Ð²ÐµÑÑ‚Ð¸Ñ†Ð¸Ð¸ Ð±ÐµÐ· Ñ€Ð¸ÑÐºÐ°\",\n",
        "    \"ÑÑ€Ð¾Ñ‡Ð½Ð¾, Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐµÐ³Ð¾Ð´Ð½Ñ\"\n",
        "]\n",
        "\n",
        "LOW_INFO_TITLES = [\"ÐŸÑ€Ð¾Ð´Ð°Ð¼\", \"Ð¥Ð¾Ñ€Ð¾ÑˆÐµÐµ\", \"ÐžÑ‚Ð´Ð°Ð¼\"]\n",
        "SPAM_TOKENS = [\"ðŸ”¥ðŸ”¥ðŸ”¥\", \"!!!\", \"Ð¢ÐžÐ›Ð¬ÐšÐž Ð¡Ð•Ð™Ð§ÐÐ¡\"]\n",
        "\n",
        "NORMAL_TEMPLATES = [\n",
        "    \"ÐŸÑ€Ð¾Ð´Ð°ÑŽ {item}. Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ {condition}. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»ÑÑ {time}. ÐŸÑ€Ð¸Ñ‡Ð¸Ð½Ð° Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ð¸ â€” {reason}.\"\n",
        "]\n",
        "\n",
        "ITEMS = [\"Ð²ÐµÐ»Ð¾ÑÐ¸Ð¿ÐµÐ´\", \"Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½\", \"Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐº\"]\n",
        "CONDITIONS = [\"Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾Ðµ\", \"Ñ…Ð¾Ñ€Ð¾ÑˆÐµÐµ\"]\n",
        "TIMES = [\"1 Ð³Ð¾Ð´\", \"6 Ð¼ÐµÑÑÑ†ÐµÐ²\"]\n",
        "REASONS = [\"Ð¿Ð¾ÐºÑƒÐ¿ÐºÐ° Ð½Ð¾Ð²Ð¾Ð³Ð¾\", \"Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ\"]\n",
        "\n",
        "\n",
        "def generate_ad(label: str) -> dict:\n",
        "    if label == \"scam\":\n",
        "        text = f\"{random.choice(SCAM_PHRASES)} {random.choice(SCAM_PHRASES)}\"\n",
        "    elif label == \"low_info\":\n",
        "        text = random.choice(LOW_INFO_TITLES)\n",
        "    elif label == \"spam\":\n",
        "        text = f\"{random.choice(SPAM_TOKENS)} ÐšÑƒÐ¿Ð¸Ñ‚ÑŒ {random.choice(SPAM_TOKENS)}\"\n",
        "    else:\n",
        "        text = random.choice(NORMAL_TEMPLATES).format(\n",
        "            item=random.choice(ITEMS),\n",
        "            condition=random.choice(CONDITIONS),\n",
        "            time=random.choice(TIMES),\n",
        "            reason=random.choice(REASONS),\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"label\": label\n",
        "    }\n",
        "\n",
        "\n",
        "def generate_dataset(n: int = 1000):\n",
        "    labels = [\"normal\", \"scam\", \"low_info\", \"spam\"]\n",
        "    return [generate_ad(random.choice(labels)) for _ in range(n)]\n",
        "\n",
        "\n",
        "def weak_label(text: str):\n",
        "    labels = {\n",
        "        \"scam\": int(\"Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€\" in text or \"Ð¿Ñ€ÐµÐ´Ð¾Ð¿Ð»Ð°Ñ‚\" in text),\n",
        "        \"spam\": int(text.count(\"!\") > 2),\n",
        "        \"low_info\": int(len(text) < 20),\n",
        "        \"adult\": 0,\n",
        "        \"illegal\": 0,\n",
        "    }\n",
        "\n",
        "    multi_hot = [labels[k] for k in RISK_LABELS]\n",
        "    risk_score = float(max(multi_hot))\n",
        "\n",
        "    return labels, risk_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7bf0826",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7bf0826",
        "outputId": "02b882f6-a225-41de-9c8b-f24abf07c8ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:44<00:00,  1.50it/s, loss=0.0919]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: loss=0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:48<00:00,  1.45it/s, loss=0.0524]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: loss=0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:47<00:00,  1.45it/s, loss=0.0258]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: loss=0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:47<00:00,  1.45it/s, loss=0.0183]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: loss=0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:47<00:00,  1.45it/s, loss=0.00879]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: loss=0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:47<00:00,  1.45it/s, loss=0.0129]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: loss=0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:48<00:00,  1.45it/s, loss=0.0063]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: loss=0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:47<00:00,  1.45it/s, loss=0.00717]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: loss=0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:48<00:00,  1.45it/s, loss=0.00214]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: loss=0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [01:48<00:00,  1.45it/s, loss=0.00238]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: loss=0.0000\n"
          ]
        }
      ],
      "source": [
        "ENCODER = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(ENCODER)\n",
        "\n",
        "raws = generate_dataset(5000)\n",
        "\n",
        "samples: List[AuditorSample] = []\n",
        "for raw in raws:\n",
        "    text = raw[\"text\"]\n",
        "    labels, score = weak_label(text)\n",
        "\n",
        "    sample = AuditorSample(\n",
        "        text=text,\n",
        "        risk_score=score,\n",
        "        risk_labels=labels,\n",
        "        source=\"synthetic\",\n",
        "    )\n",
        "    samples.append(sample)\n",
        "\n",
        "dataset = AuditorDataset(samples, tokenizer)\n",
        "model = train_auditor(dataset, ENCODER)\n",
        "\n",
        "torch.save(model.state_dict(), \"auditor.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
