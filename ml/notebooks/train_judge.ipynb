{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc476c24",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da18f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20087bc8",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6033abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        num_classes: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"probs\": probs\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307b520",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26eddafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X)\n",
    "        self.y = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a61718",
   "metadata": {},
   "source": [
    "## Generation synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88202549",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERDICTS = {\"publish\": 0, \"revise\": 1, \"reject\": 2}\n",
    "RISK_LABELS = [\"scam\", \"adult\", \"illegal\", \"spam\", \"low_info\"]\n",
    "\n",
    "\n",
    "def build_judge_features(auditor, quality, meta=None):\n",
    "    features = [\n",
    "        auditor[\"risk_score\"],\n",
    "        auditor.get(\"uncertainty\", 0.0),\n",
    "        quality[\"quality_score\"],\n",
    "        quality.get(\"confidence\", 1.0),\n",
    "    ]\n",
    "\n",
    "    # Quality aspects\n",
    "    for k in [\"informativeness\", \"clarity\", \"completeness\", \"persuasion\"]:\n",
    "        features.append(quality[\"aspects\"].get(k, 0.0))\n",
    "\n",
    "    # Optional meta-features\n",
    "    if meta:\n",
    "        features.append(meta.get(\"text_length\", 0) / 1000)\n",
    "        features.append(meta.get(\"embedding_norm\", 0.0))\n",
    "\n",
    "    return np.array(features, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def generate_fake_auditor():\n",
    "    risk = np.random.beta(2, 5)\n",
    "    label_probs = np.random.dirichlet(np.ones(len(RISK_LABELS)))\n",
    "    entropy = -(label_probs * np.log(label_probs + 1e-8)).sum()\n",
    "\n",
    "    return {\n",
    "        \"risk_score\": float(risk),\n",
    "        \"risk_labels\": dict(zip(RISK_LABELS, label_probs)),\n",
    "        \"uncertainty\": float(entropy)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_fake_quality():\n",
    "    aspects = {\n",
    "        \"informativeness\": np.random.uniform(0.2, 1.0),\n",
    "        \"clarity\": np.random.uniform(0.2, 1.0),\n",
    "        \"completeness\": np.random.uniform(0.2, 1.0),\n",
    "        \"persuasion\": np.random.uniform(0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    quality = np.mean(list(aspects.values()))\n",
    "    confidence = 1 - np.std(list(aspects.values()))\n",
    "\n",
    "    return {\n",
    "        \"quality_score\": float(quality),\n",
    "        \"aspects\": aspects,\n",
    "        \"confidence\": float(confidence)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_judge_sample():\n",
    "    auditor = generate_fake_auditor()\n",
    "    quality = generate_fake_quality()\n",
    "\n",
    "    meta = {\n",
    "        \"text_length\": np.random.randint(20, 2000),\n",
    "        \"embedding_norm\": np.random.uniform(5, 15)\n",
    "    }\n",
    "\n",
    "    features = build_judge_features(auditor, quality, meta)\n",
    "\n",
    "    if auditor[\"risk_score\"] > 0.7:\n",
    "        label = VERDICTS[\"reject\"]\n",
    "    elif quality[\"quality_score\"] > 0.75 and auditor[\"risk_score\"] < 0.3:\n",
    "        label = VERDICTS[\"publish\"]\n",
    "    else:\n",
    "        label = VERDICTS[\"revise\"]\n",
    "\n",
    "    return np.array(features, dtype=\"float32\"), label\n",
    "\n",
    "\n",
    "def generate_judge_dataset(n=5000):\n",
    "    X, y = [], []\n",
    "    for _ in range(n):\n",
    "        f, l = generate_judge_sample()\n",
    "        X.append(f)\n",
    "        y.append(l)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a28e561",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c8b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_judge(dataset, input_dim: int):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = JudgeModel(input_dim=input_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    epochs = 10\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        loop = tqdm.tqdm(loader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "        for x, y in loop:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            out = model(x)\n",
    "            loss = criterion(out[\"logits\"], y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: loss={epoch_loss / len(loader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c75cd3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5b73e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z9/19wy68_x711cm327qjf79_pc0000gn/T/ipykernel_17170/3878342614.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  self.X = torch.tensor(X)\n",
      "Epoch 1: 100%|██████████| 157/157 [00:00<00:00, 502.09it/s, loss=0.099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.3266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 157/157 [00:00<00:00, 884.62it/s, loss=0.0754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=0.2685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 157/157 [00:00<00:00, 864.62it/s, loss=0.0639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=0.2489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 157/157 [00:00<00:00, 866.10it/s, loss=0.106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss=0.2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 157/157 [00:00<00:00, 890.40it/s, loss=0.107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss=0.1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 157/157 [00:00<00:00, 812.49it/s, loss=0.0213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss=0.1552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 157/157 [00:00<00:00, 745.63it/s, loss=0.235] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss=0.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 157/157 [00:00<00:00, 912.67it/s, loss=0.0583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss=0.1239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 157/157 [00:00<00:00, 909.05it/s, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss=0.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 157/157 [00:00<00:00, 911.67it/s, loss=0.0651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss=0.1113\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_judge_dataset()\n",
    "dataset = JudgeDataset(X, y)\n",
    "\n",
    "model = train_judge(dataset, input_dim=X[0].shape[0])\n",
    "torch.save(model.state_dict(), \"judge.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
